{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef, auc, roc_curve, log_loss, accuracy_score, precision_score, f1_score, recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 0\n",
    "\n",
    "trainer = pd.read_csv(f\"labels/train_subset{subset}.csv\")\n",
    "tester = pd.read_csv(f\"labels/test_subset{subset}.csv\")\n",
    "validator = pd.read_csv(f\"labels/val_subset{subset}.csv\")\n",
    "\n",
    "trainer[\"Filename\"] = \"images/\" + trainer[\"Filename\"]\n",
    "tester[\"Filename\"] = \"images/\" + tester[\"Filename\"]\n",
    "validator[\"Filename\"] = \"images/\" + validator[\"Filename\"]\n",
    "\n",
    "label_map = {\n",
    "    0: \"Chinee_Apple\",\n",
    "    1: \"Lantana\",\n",
    "    2: \"Parkinsonia\",\n",
    "    3: \"Parthenium\",\n",
    "    4: \"Prickly_Acacia\",\n",
    "    5: \"Rubber_Vine\",\n",
    "    6: \"Siam_Weed\",\n",
    "    7: \"Snake_Weed\",\n",
    "    8: \"Negative\"\n",
    "}\n",
    "\n",
    "trainer[\"Label\"] = trainer[\"Label\"].map(label_map)\n",
    "tester[\"Label\"] = tester[\"Label\"].map(label_map)\n",
    "validator[\"Label\"] = validator[\"Label\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "datagen = ImageDataGenerator(rescale=1/255, rotation_range = 30)\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(\n",
    "    trainer,\n",
    "    x_col = \"Filename\",\n",
    "    y_col = \"Label\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_dataframe(\n",
    "    validator,\n",
    "    x_col = \"Filename\",\n",
    "    y_col = \"Label\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_dataframe(\n",
    "    tester,\n",
    "    x_col = \"Filename\",\n",
    "    y_col = \"Label\",\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=layers.Input(shape=(224, 224, 3))\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True) #stop early if the val loss doesn't decrease beyond 7 epochs and restore the model to its best weights\n",
    "# if val loss increases, its a sign of overfitting - the model's memorising the train data\n",
    "\n",
    "reducelronplateau = tf.keras.callbacks.ReduceLROnPlateau( #if the performance isn't improving, the model reduces LR\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, #reduce by this factor. eg 0.5x will half the LR\n",
    "    patience=3, #how many epochs long the performance stagnation will last before LR forced to reduce\n",
    "    verbose=1,\n",
    "    min_lr=1e-5  #minimum LR- prevents too much reduction\n",
    ") #done to ensure that the model reaches its local minima instead of continually overshooting it\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=50, #the model won't likely complete all epochs due to early stopping\n",
    "    validation_data = val_gen,\n",
    "    callbacks = [early_stop, reducelronplateau]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='train accuracy') #plots training and val accuracy v/s epoch\n",
    "plt.plot(history.history['val_accuracy'], label='val Accuracy')\n",
    "plt.title('accuracy Curves')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='train Loss') #plots training and val loss v/s epoch\n",
    "plt.plot(history.history['val_loss'], label='val Loss')\n",
    "plt.title('loss Curves')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabalistics = model.predict(test_gen)\n",
    "\n",
    "print(\"example of the cnn output showing probabilities:\")\n",
    "print(probabalistics[0]) #softmax output for probability values of classes\n",
    "\n",
    "predictions = np.argmax(probabalistics, axis=1) #converts row into single value of highest prob. index = class ID\n",
    "real_labels = np.argmax(test_gen, axis=1) #same here, except only 0s and 1s\n",
    "\n",
    "indexed_items = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(classification_report(real_labels, predictions)) #classification report and confusion matrix\n",
    "\n",
    "cm = confusion_matrix(real_labels, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"inferno\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.xticks(np.arange(10) + 0.5, indexed_items, rotation=90)\n",
    "plt.yticks(np.arange(10) + 0.5, indexed_items, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(cm, 0) #removing all correctly classified examples - in the diagonal\n",
    "sns.heatmap(cm, annot=True, cmap=\"inferno\")\n",
    "plt.xticks(np.arange(10) + 0.5, indexed_items, rotation=90)\n",
    "plt.yticks(np.arange(10) + 0.5, indexed_items, rotation=0)\n",
    "plt.title(\"Misclassification report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.where(predictions != real_labels)[0] #displaying the first 5 images where the model misclassified.\n",
    "for i in errors[:5]:\n",
    "    plt.imshow(test_gen[i], cmap='gray')\n",
    "    plt.title(f\"True: {indexed_items[real_labels[i]]}, Pred: {indexed_items[predictions[i]]}\")\n",
    "    plt.show()\n",
    "    \n",
    "#the classes tshirt and shirt are very similar. the model is expected to make some errors here. similar with ankle boots and sandals, and pullovers and coats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df287a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matthew's Correlation Coefficient : {matthews_corrcoef(real_labels, predictions)}\")\n",
    "print(f\"Accuracy : {accuracy_score(real_labels, predictions)}\")\n",
    "print(f\"f1 score : {f1_score(real_labels, predictions, average='weighted')}\")\n",
    "print(f\"Log loss : {log_loss(real_labels, probabalistics)}\")\n",
    "print(f\"Recall : {recall_score(real_labels, predictions, average='weighted')}\")\n",
    "print(f\"Precision : {precision_score(real_labels, predictions, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import label_binarize\n",
    "\n",
    "real_bin = label_binarize(real_labels, classes=np.arange(10))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    false_p_rate, true_p_rate, thresholds = roc_curve(real_bin[:, i], probabalistics[:, i])\n",
    "    \n",
    "    roc_auc = auc(false_p_rate, true_p_rate) #higher area under curve - straighter the lines - lesser the FPR\n",
    "    \n",
    "    plt.plot(false_p_rate, true_p_rate, label=f\"{indexed_items[i]} - AUC: {roc_auc:.2f}\")\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curves (1 v/s rest)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
