{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning a BERT Encoder for Named Entity Recognition on CoNLL2003\n",
        "\n",
        "Implemented for the NLP Cryptonite Research AI Taskphase by Pratham Shah - 240905614."
      ],
      "metadata": {
        "id": "HxhdMRhN54YZ"
      },
      "id": "HxhdMRhN54YZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing seqeval"
      ],
      "metadata": {
        "id": "B_EYeAp26PWx"
      },
      "id": "B_EYeAp26PWx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m4H4ZgBVfoP",
        "outputId": "1684a185-9ed8-42c7-8d1c-8148889f79f1"
      },
      "id": "8m4H4ZgBVfoP",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Imports"
      ],
      "metadata": {
        "id": "xl1CLezH6TZK"
      },
      "id": "xl1CLezH6TZK"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aae0b039",
      "metadata": {
        "id": "aae0b039"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "from seqeval.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Labels"
      ],
      "metadata": {
        "id": "ISrZ6bztEE2A"
      },
      "id": "ISrZ6bztEE2A"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "05eff59f",
      "metadata": {
        "id": "05eff59f"
      },
      "outputs": [],
      "source": [
        "NER_labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
        "label_id_map = {label: i for i, label in enumerate(NER_labels)}\n",
        "id_label_map = {i: label for label, i in label_id_map.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CoNLL-2003 Data"
      ],
      "metadata": {
        "id": "neT8od2jEQhT"
      },
      "id": "neT8od2jEQhT"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8790bd33",
      "metadata": {
        "id": "8790bd33"
      },
      "outputs": [],
      "source": [
        "def read_conll_file(file_path):\n",
        "    tokens, tags = [], []\n",
        "    temp_tokens, temp_tags = [], []\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if temp_tokens:\n",
        "                    tokens.append(temp_tokens)\n",
        "                    tags.append(temp_tags)\n",
        "                    temp_tokens, temp_tags = [], []\n",
        "                continue\n",
        "            word, pos, chunk, ner = line.split()\n",
        "            temp_tokens.append(word)\n",
        "            temp_tags.append(label_id_map[ner])\n",
        "    return tokens, tags\n",
        "\n",
        "train_tokens, train_tags = read_conll_file(\"train.txt\")\n",
        "val_tokens, val_tags = read_conll_file(\"valid.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer, Alignment and Model"
      ],
      "metadata": {
        "id": "8sSYXuAgJmGW"
      },
      "id": "8sSYXuAgJmGW"
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"bert-large-uncased\""
      ],
      "metadata": {
        "id": "iz4EDx_HK1O5"
      },
      "id": "iz4EDx_HK1O5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7aa25e1c",
      "metadata": {
        "id": "7aa25e1c"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "def tokenize_and_align_labels(tokens_list, tags_list):\n",
        "    encodings = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "    for tokens, labels in zip(tokens_list, tags_list):\n",
        "        tokenized = tokenizer(tokens, is_split_into_words=True, truncation=True, padding='max_length', max_length=128)\n",
        "        word_ids = tokenized.word_ids()\n",
        "\n",
        "        aligned_labels = []\n",
        "        prev_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)\n",
        "            elif word_idx != prev_word_idx:\n",
        "                aligned_labels.append(labels[word_idx])\n",
        "            else:\n",
        "                aligned_labels.append(-100)\n",
        "            prev_word_idx = word_idx\n",
        "\n",
        "        encodings[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
        "        encodings[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
        "        encodings[\"labels\"].append(aligned_labels)\n",
        "\n",
        "    return encodings\n",
        "\n",
        "train_enc = tokenize_and_align_labels(train_tokens, train_tags)\n",
        "val_enc = tokenize_and_align_labels(val_tokens, val_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER Dataset creation"
      ],
      "metadata": {
        "id": "6uEDgb3wJ762"
      },
      "id": "6uEDgb3wJ762"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e345d913",
      "metadata": {
        "id": "e345d913"
      },
      "outputs": [],
      "source": [
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "train_dataset = NERDataset(train_enc)\n",
        "val_dataset = NERDataset(val_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model"
      ],
      "metadata": {
        "id": "TSJATuZTJ_3B"
      },
      "id": "TSJATuZTJ_3B"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8f547758",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f547758",
        "outputId": "2b83825b-b9f3-4d6c-faf8-2fcb6a2f8926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model, num_labels=len(NER_labels))\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for Metrics Calculation"
      ],
      "metadata": {
        "id": "G8DBFgUxKFcQ"
      },
      "id": "G8DBFgUxKFcQ"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "58f5829b",
      "metadata": {
        "id": "58f5829b"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = np.argmax(predictions, axis=-1)\n",
        "    true_preds, true_labels = [], []\n",
        "    for pred_seq, label_seq in zip(preds, labels):\n",
        "        temp_preds, temp_labels = [], []\n",
        "        for p, l in zip(pred_seq, label_seq):\n",
        "            if l != -100:\n",
        "                temp_preds.append(id_label_map[p])\n",
        "                temp_labels.append(id_label_map[l])\n",
        "        true_preds.append(temp_preds)\n",
        "        true_labels.append(temp_labels)\n",
        "    report = classification_report(true_labels, true_preds, output_dict=True)\n",
        "    return {\n",
        "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Training Arguments"
      ],
      "metadata": {
        "id": "KGTk93MYKJXr"
      },
      "id": "KGTk93MYKJXr"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "33e4ea41",
      "metadata": {
        "id": "33e4ea41"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    report_to=[],\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    bert_dropout_rate = 0.2,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining The Trainer"
      ],
      "metadata": {
        "id": "RuTsu7j8KND-"
      },
      "id": "RuTsu7j8KND-"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "11b307ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11b307ac",
        "outputId": "48fcf2cf-f4b5-4c71-d604-5a918e5f3766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25-3963635074.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluation"
      ],
      "metadata": {
        "id": "lYZz1CBqKQt6"
      },
      "id": "lYZz1CBqKQt6"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "89ffc8eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "89ffc8eb",
        "outputId": "30e7f9a9-5ac3-4666-e0c3-617bd2b751fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3284' max='4685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3284/4685 1:19:31 < 33:56, 0.69 it/s, Epoch 3.50/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.168500</td>\n",
              "      <td>0.058752</td>\n",
              "      <td>0.911016</td>\n",
              "      <td>0.920020</td>\n",
              "      <td>0.915205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.054700</td>\n",
              "      <td>0.048024</td>\n",
              "      <td>0.919737</td>\n",
              "      <td>0.941404</td>\n",
              "      <td>0.930241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.029100</td>\n",
              "      <td>0.043323</td>\n",
              "      <td>0.938512</td>\n",
              "      <td>0.952349</td>\n",
              "      <td>0.945324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.022700</td>\n",
              "      <td>0.049622</td>\n",
              "      <td>0.944864</td>\n",
              "      <td>0.952181</td>\n",
              "      <td>0.948446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>0.043856</td>\n",
              "      <td>0.947412</td>\n",
              "      <td>0.953696</td>\n",
              "      <td>0.950490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.051002</td>\n",
              "      <td>0.943119</td>\n",
              "      <td>0.953864</td>\n",
              "      <td>0.948436</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4685' max='4685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4685/4685 1:56:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.168500</td>\n",
              "      <td>0.058752</td>\n",
              "      <td>0.911016</td>\n",
              "      <td>0.920020</td>\n",
              "      <td>0.915205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.054700</td>\n",
              "      <td>0.048024</td>\n",
              "      <td>0.919737</td>\n",
              "      <td>0.941404</td>\n",
              "      <td>0.930241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.029100</td>\n",
              "      <td>0.043323</td>\n",
              "      <td>0.938512</td>\n",
              "      <td>0.952349</td>\n",
              "      <td>0.945324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.022700</td>\n",
              "      <td>0.049622</td>\n",
              "      <td>0.944864</td>\n",
              "      <td>0.952181</td>\n",
              "      <td>0.948446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>0.043856</td>\n",
              "      <td>0.947412</td>\n",
              "      <td>0.953696</td>\n",
              "      <td>0.950490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.051002</td>\n",
              "      <td>0.943119</td>\n",
              "      <td>0.953864</td>\n",
              "      <td>0.948436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.006800</td>\n",
              "      <td>0.047513</td>\n",
              "      <td>0.940695</td>\n",
              "      <td>0.953022</td>\n",
              "      <td>0.946803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.050465</td>\n",
              "      <td>0.948189</td>\n",
              "      <td>0.955380</td>\n",
              "      <td>0.951769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.050660</td>\n",
              "      <td>0.946417</td>\n",
              "      <td>0.955548</td>\n",
              "      <td>0.950949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='217' max='217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [217/217 01:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.050464510917663574, 'eval_precision': 0.9481892932699343, 'eval_recall': 0.9553796935511029, 'eval_f1': 0.9517690819625958, 'eval_runtime': 82.9272, 'eval_samples_per_second': 41.796, 'eval_steps_per_second': 2.617, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b3c62389",
      "metadata": {
        "id": "b3c62389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa1791e-4391-4e35-b2f4-7deaabf8e887"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./bert-ner-model2/tokenizer_config.json',\n",
              " './bert-ner-model2/special_tokens_map.json',\n",
              " './bert-ner-model2/vocab.txt',\n",
              " './bert-ner-model2/added_tokens.json',\n",
              " './bert-ner-model2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# 12. Save Final Model\n",
        "trainer.save_model(\"./bert-ner2\")\n",
        "tokenizer.save_pretrained(\"./bert-ner-model2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r bert-ner2.zip bert-ner2\n",
        "!zip -r bert-ner-model2.zip bert-ner-model2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wGkMi3_UOJ5",
        "outputId": "538d76c5-daab-4d5e-9198-b51add466623"
      },
      "id": "5wGkMi3_UOJ5",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bert-ner2/ (stored 0%)\n",
            "  adding: bert-ner2/training_args.bin (deflated 52%)\n",
            "  adding: bert-ner2/model.safetensors (deflated 7%)\n",
            "  adding: bert-ner2/vocab.txt (deflated 53%)\n",
            "  adding: bert-ner2/tokenizer.json (deflated 71%)\n",
            "  adding: bert-ner2/config.json (deflated 56%)\n",
            "  adding: bert-ner2/special_tokens_map.json (deflated 42%)\n",
            "  adding: bert-ner2/tokenizer_config.json (deflated 75%)\n",
            "  adding: bert-ner-model2/ (stored 0%)\n",
            "  adding: bert-ner-model2/vocab.txt (deflated 53%)\n",
            "  adding: bert-ner-model2/tokenizer.json (deflated 71%)\n",
            "  adding: bert-ner-model2/special_tokens_map.json (deflated 42%)\n",
            "  adding: bert-ner-model2/tokenizer_config.json (deflated 75%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('bert-ner2.zip')"
      ],
      "metadata": {
        "id": "qdt2vhmIXy4M"
      },
      "id": "qdt2vhmIXy4M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Uf-erLTZMoD"
      },
      "id": "6Uf-erLTZMoD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}