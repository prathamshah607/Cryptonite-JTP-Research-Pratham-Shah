{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6eda0d",
   "metadata": {},
   "source": [
    "# WordPiece and BPE Tokenizers with Normalising and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173702a6-0ebd-43e6-b670-4fdd022311bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['Un', 'char', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'determined', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'pro', 'c', 'ess']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "#importing the tokenisers\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "#importing the trainers for the tokenisers\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "#pre tokenise the string by splitting at whitespaces (split it into words)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "#these tokens are not be edited, and must be included in the final token corpus\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[OOV]\"])\n",
    "tokenizer.train([\"random_text_corpus.txt\"], trainer)\n",
    "\n",
    "#encode this sample string\n",
    "output = tokenizer.encode(\"Uncharacteristically, she pre-determined the outcome before initiating the process\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed324ff-0290-498c-aa6a-1bca015794aa",
   "metadata": {},
   "source": [
    "## Increasingly complicated storytelling:\n",
    "['nch', 'ar', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'de', 'ter', 'min', 'ed', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'pro', 'cess']\n",
    "\n",
    "['U', 'nch', 'ar', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'det', 'er', 'min', 'ed', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'p', 'ro', 'c', 'ess']\n",
    "\n",
    "['U', 'nch', 'ar', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'det', 'er', 'min', 'ed', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'pro', 'c', 'ess']\n",
    "\n",
    "## Adding some scientifc text:\n",
    "['Un', 'ch', 'ar', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'determined', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'pro', 'c', 'ess']\n",
    "\n",
    "['Un', 'ch', 'ar', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'determined', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'pro', 'c', 'ess']\n",
    "\n",
    "## Adding some random conversational tones and dialogues:\n",
    "['Un', 'char', 'ac', 'ter', 'ist', 'ically', ',', 'she', 'pre', '-', 'determined', 'the', 'out', 'come', 'before', 'in', 'it', 'i', 'ating', 'the', 'pro', 'c', 'ess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363aea56-110c-4863-9c98-5f8a2283433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Un', 'character', 'istically', ',', 'Ġshe', 'Ġpre', '-', 'd', 'etermined', 'Ġthe', 'Ġoutcome', 'Ġbefore', 'Ġinitiating', 'Ġthe', 'Ġprocess']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # gpt2 uses bpe\n",
    "tokens = tokenizer.tokenize(\"Uncharacteristically, she pre-determined the outcome before initiating the process\")\n",
    "print(tokens)\n",
    "\n",
    "#the G indicates leading whitespace. it can be removed if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2377d9-3a5c-4e65-b920-c9e85b8868cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['unc', '##h', '##ar', '##act', '##er', '##istic', '##ally', ',', 'she', 'pre', '-', 'determined', 'the', 'outcom', '##e', 'before', 'in', '##it', '##ia', '##ting', 'the', 'pr', '##oc', '##ess']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents, Sequence\n",
    "\n",
    "#for all unknown tokens in test sets itll return [UNK]\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "#normalising helps reduce vocabulary. NFD separates accents, to lower case, then removes accents\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "#pre tokenize by splitting words\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "tokenizer.train([\"random_text_corpus.txt\"], trainer)\n",
    "\n",
    "output = tokenizer.encode(\"Uncharacteristically, she pre-determined the outcome before initiating the process\")\n",
    "print(output.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18f4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
